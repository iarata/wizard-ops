# Cloud Build configuration for training pipeline
# Triggers: Manual or on changes to training code/configs
# This pipeline:
# 1. Builds the training Docker image to Artifact Registry
# 2. Runs training on Cloud Build
# 3. Pushes checkpoints to GCS via DVC
#
# Settings are read from configs/config.yaml where possible

substitutions:
  _REGION: "europe-west4"
  _BUCKET: "dtu-kfc-bucket"
  _ARTIFACT_REGISTRY: "europe-west4-docker.pkg.dev/${PROJECT_ID}/container-registry"
  _TRAIN_IMAGE: "${_ARTIFACT_REGISTRY}/wizard-ops-train"
  # Training overrides (defaults come from config.yaml)
  _EXPERIMENT_NAME: ""
  _BACKBONE: ""
  _MAX_EPOCHS: ""
  _BATCH_SIZE: ""
  _FAST_DEV_RUN: ""

steps:
  # Step 0: Pull the latest image to use as cache (ignore failure if it doesn't exist)
  - name: "gcr.io/cloud-builders/docker"
    id: "pull-cache"
    entrypoint: "bash"
    args:
      - "-c"
      - "docker pull ${_ARTIFACT_REGISTRY}/wizard-ops-train:latest || exit 0"

  # Step 1: Build training Docker image with layer caching
  - name: "gcr.io/cloud-builders/docker"
    id: "build-train-image"
    args:
      - "build"
      - "--cache-from"
      - "${_ARTIFACT_REGISTRY}/wizard-ops-train:latest"
      - "-f"
      - "dockerfiles/train.dockerfile"
      - "-t"
      - "${_ARTIFACT_REGISTRY}/wizard-ops-train:${SHORT_SHA}"
      - "-t"
      - "${_ARTIFACT_REGISTRY}/wizard-ops-train:latest"
      - "."
    waitFor: ["pull-cache"]

  # Step 2: Push training image to Artifact Registry (push both tags in parallel)
  - name: "gcr.io/cloud-builders/docker"
    id: "push-train-image"
    args:
      - "push"
      - "--all-tags"
      - "${_ARTIFACT_REGISTRY}/wizard-ops-train"
    waitFor: ["build-train-image"]

  # Step 3: Run training container
  # The container reads config.yaml for defaults, env vars override specific settings
  - name: "${_ARTIFACT_REGISTRY}/wizard-ops-train:${SHORT_SHA}"
    id: "run-training"
    env:
      - "EXPERIMENT_NAME=${_EXPERIMENT_NAME}"
      - "BACKBONE=${_BACKBONE}"
      - "MAX_EPOCHS=${_MAX_EPOCHS}"
      - "BATCH_SIZE=${_BATCH_SIZE}"
      - "FAST_DEV_RUN=${_FAST_DEV_RUN}"
      - "GOOGLE_APPLICATION_CREDENTIALS=/workspace/credentials.json"
    secretEnv: ["GCS_CREDENTIALS"]
    entrypoint: "bash"
    args:
      - "-c"
      - |
        echo "$$GCS_CREDENTIALS" > /workspace/credentials.json
        export GOOGLE_APPLICATION_CREDENTIALS=/workspace/credentials.json
        /app/train_entrypoint.sh
    waitFor: ["push-train-image"]

  # Step 4: Store the experiment metadata for downstream pipelines
  - name: "gcr.io/cloud-builders/gsutil"
    id: "store-metadata"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        echo '{"experiment":"${_EXPERIMENT_NAME}","backbone":"${_BACKBONE}","commit":"${SHORT_SHA}","timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' \
        | gsutil cp - gs://${_BUCKET}/metadata/latest_training.json
    waitFor: ["run-training"]

images:
  - "${_ARTIFACT_REGISTRY}/wizard-ops-train:${SHORT_SHA}"
  - "${_ARTIFACT_REGISTRY}/wizard-ops-train:latest"

availableSecrets:
  secretManager:
    - versionName: "projects/${PROJECT_ID}/secrets/gcs-credentials/versions/latest"
      env: "GCS_CREDENTIALS"

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: "E2_HIGHCPU_8"
  diskSizeGb: 100

timeout: "7200s" # 2 hours for training
